\section{Data Analysis}

TODO : Replace 2mois by \textit{2 Months}
       Replace 5mois by \textit{5 Months}

The main goal of this section is to extract knowledge from the current movements to guide the design of the storage controller. 

\subsection{Data Source}

This source of the data was the HPC database from InfluxDB. The table \textit{transfer\_history} contains the transfer information for transfers from the CEPH to the QBoxes. This database has a few restrictions as mentioned below

\begin{enumerate}
    \item The retention policies were present, but the continuous queries were very not set up. So there are chunks of time where the data is not present.
    \item The transfers from the QBoxes to the CEPH were not taken into consideration. These transfers are the result datasets that result from the computation at the QMobos and are sent back to the CEPH.
    \item The transfers that are done manually by the customers from the QBoxes to their own servers is also not considered (possibly for privacy reasons).
    \item The datasets do have a unique id that can be used to identify each one. For this analysis. 
\end{enumerate}

The restrictions were recently resolved, but not enough data is present to find something conclusive. The data analysis in the following article is done on a traces in the time from Sep 2018 to Jan 2019 (which is denoted by \textit{5 Months}) and 4th March 2019 to 18 March 2019 (which is denoted by \textit{2 Weeks}). The data is present in the \textit{five\_year} and \textit{two\_months} retention policies respectively and extracted as a CSV file without aggregation functions using the \textit{chronograf} UI.

\section{Basic Statistics}

The \textit{5 Months} retention policy has 35k entries and the 2mois has 12k entries. This does not scale well. The density in 2mois is very large as compared to the density in 5mois. Also, the continuous query does not down sample the data.

The 5mois data has 20 QBoxes as compared to 24 Qboxes in the 2mois data. This says that there were possible new customers due to which there was a density change.

In both the datasets, the number of UP transfers and the number of failed transfers is less than 1\%. Looking at the failed transfers, it is easy to see  that the failures occurred in a very short amount of time. This shows high correlation between the failures. An external factor can be the cause.

\subsection{Data Size and Transfer time Distribution}

Since the datasets do not have a unique id, the way the datasets are differentiated is based of the assumption that the probability of two different datasets being of same size is very low. Based on this, the dataset 5mois has 417 unique datasets being used and the 2mois datasets has 270 datasets.

If the datasets are extracted using the \textit{qarnot-extractor} which aggregates all the datasets from all the retention policies, then there are 870 entries in the whole set. The following plots show the distribution of sizes.

The figure \ref{fig:size_unique} shows the unique sizes present in the 5mois data. The y-axis is the log of the size to have a clear graph. most of the data lies between 1MB and 100MB. Files greater than 1GB are very less for now.

\begin{figure}[ht]
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.3]{images/data_analysis_images/01_size_unique.png}
    \caption{Histogram of unique sizes of the datasets in 5mois}
    \label{fig:size_unique}
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.3]{images/data_analysis_images/02_size_all.png}
    \caption{Histogram of unique sizes of the datasets in 5mois}
    \label{fig:size_all}
  \end{minipage}
\end{figure}

The figure \ref{fig:size_all} shows the sizes of the data that is requested in the 5mois data. The standard deviation of the graph reduces drastically and most of the data that is requested lies between 10MB and 100MB. This is good information and can be used for speculative algorithms.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.6]{images/data_analysis_images/03_transfer_time.png}
    \caption{Histogram for transfer time in 5mois}
    \label{fig:transfer_time}
\end{figure}

For the transfer time, \ref{fig:transfer_time} shows the distribution of the time taken by the transfers to occur. More than 98\% transfers are less than 20s. Given that most of the datasets requested are between 10MB and 100MB, this shows that the bandwidth values are very high.

\subsection{Mapping of jobs and data}

Jobs are submitted by the user and might use more than one datasets. Some datasets can be common to multiple jobs. This can be useful in case for speculative behaviour in case the dataset is very popular. Network traffic can be mitigated effectively if this information is known.

In the 5mois data, there are 17k unique jobs. 54\% of the jobs use more than one dataset and 16\% of the datasets are used by more than one job. This result is not consistent with the 2mois data. For the 2mois, the values change to 33\% and 33\%. This value is based on what type of customer are present in the system and how what type of jobs they have. Hence this value is bound to change. There is not enough data to see the trends in these values. The figures \ref{fig:job_to_data} and \ref{fig:job_to_data} shows how plot of the count of datasets that use a fixed number of jobs and the count of jobs that use a fixed number of datasets. These figures are plotted with the 5mois data.

\begin{figure}[H]
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.30]{images/data_analysis_images/04_job_to_data.png}
    \caption{How many times a dataset is used by a job}
    \label{fig:job_to_data}
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.30]{images/data_analysis_images/05_data_to_job.png}
    \caption{How many times a job uses a dataset}
    \label{fig:job_to_data}
  \end{minipage}
\end{figure}

\subsection{QBox Load analysis}

For the 20 QBoxes that are present in the 5mois data, this section shows how the transfer count and the transfer size load changes with QBoxes. The figures \ref{fig:qbox_count} and \ref{fig:qbox_size} are plotted by grouping the data on days and QBoxes and then finding the mean and deviation of the count and the total size of the transfer respectively. This shows what is the average load in terms of the transfers (and effectively the jobs) in the specific QBox. The variation in the graph shows that some Qboxes have very less data that is transferred making them nearly unused. On the other hand, some Qboxes have a lot of data (possibly more than 500GB) nearly taking up the entire space of the QBoxes.

\begin{figure}[H]
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.30]{images/data_analysis_images/06_qbox_size.png}
    \caption{Average count of transfers over each day per QBox}
    \label{fig:qbox_count}
  \end{minipage}
  \hspace{0.5cm}
  \begin{minipage}[b]{0.5\linewidth}
    \centering
    \includegraphics[scale=0.30]{images/data_analysis_images/07_qbox_count.png}
    \caption{Average size of transfers over each day per QBox}
    \label{fig:qbox_size}
  \end{minipage}
\end{figure}

A similar graph \ref{fig:hourly} shows the count of transfers that the whole system had per hour. This plot clearly reflects the large number of background tasks that run at the night between 12 and 1 AM. These are tasks that do not really need much data and hence, there is no spike visible in the hourly analysis of transfer size. The high variation at 1 PM shows that the customer start executing jobs on the QRads around 10 AM and till 4 PM. The standard deviation is very high in this range. These graphs are consistent with what was expected in the system and hence form a reliable way to analyse the scope of improvement.

\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.6]{images/data_analysis_images/08_hourly.png}
    \caption{Average number of transfers over each day per hour}
    \label{fig:hourly}
\end{figure}

\subsection{Cache Analysis}

Based on the high value of transfers, low datasets and very low number of QBoxes, it is important to predict when cache issues can start occurring in the QBoxes. Based on the QBoxes and the related datasets, we can define a relation as follows. A QBox is related to a dataset if that dataset has been in the qbox at least once in the whole trace.

\begin{enumerate}
    \item Number of Qboxes : 20
    \item Number of Unique datasets : 413
    \item Possible valid relations : 413*20 = 8260
    \item Valid number of relations as found in traces : 1206
\end{enumerate}

Now, all the transfers are categorised among these 1206 relations. Then the number of times a cache is found is counted. The first time a cache was created is a mandatory action. So if this one action is disregarded, the cache utilisation percentage turns out to be 77\% for the 5mois and 41\% for 2mois. This is inconsistent, but these values are very high. More the customers, more the data, worse the cache utilisation gets. The current data present in the CEPH is 1.2TB. Given that every QBox has 1TB space, data cache miss is rare. The cache percentages show that.

To make this precise, the information on how many days a dataset is still used by a job is needed. The popularity of the dataset is just a graph showing the number of time it has been requested in a day. The first day it is requested is normalized to \textit{Day 0}. And then the average for all these datasets is taken to get the \textit{Data Popularity Graph} \ref{fig:popular}.

The figure \ref{fig:popular} shows the popularity of the dataset. There are three points of interest. The point at 80 days where the standard deviation is very less. This is the point where most of the datasets loose their popularity. In the worst case, the data is never used beyond 120 days (4 months). This worst case figure is used in the calculations to see the scalability of this system. The third point is at day 100 where suddenly a some datasets get used a lot. There is no particular reason which explains this for now.

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{images/data_analysis_images/10_popularity.png}
    \caption{Data Popularity Graph}
    \label{fig:popular}
\end{figure}


\begin{enumerate}
    \item Average number of Qboxes every dataset : 3
    \item Total size per Qbox : 1TB
    \item Average data popularity : 120 days (worst case)
    \item Average unique datasets requested every day : 44
    \item Average unique data size : 300MB
    \item \textbf{Average size per Qbox used (assuming worst case popularity) : 198 GB}
\end{enumerate}

The heatmap \ref{fig:heatmap} shows the total number of transfers that occur in a qbox in an hour in the 5mois data. The average of this is 4 which is very less. Given that 77\% of the data is cached, this is not surprising, but this does not by any means scale.

\begin{figure}
    \centering
    \includegraphics[scale=0.6]{images/data_analysis_images/09_heatmap.png}
    \caption{Count of non-zero transfers per qbox per hour}
    \label{fig:heatmap}
\end{figure}